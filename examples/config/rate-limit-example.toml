# Rate Limiting Configuration Example
# This file demonstrates how to configure rate limiting in huginn-proxy

# Basic server configuration
listen = "0.0.0.0:7000"

# Backend servers
[[backends]]
address = "backend-a:9000"

[[backends]]
address = "backend-b:9000"

# Security configuration with rate limiting
[security]
max_connections = 512

# Global rate limiting configuration
# Applies to all routes unless overridden
[security.rate_limit]
enabled = true
requests_per_second = 1000  # Maximum requests per second
burst = 2000                # Maximum burst size (allows temporary spikes)
window_seconds = 1          # Time window in seconds
limit_by = "ip"             # Rate limit by client IP address

# Alternative limit_by options:
# limit_by = "ip"            - Rate limit by client IP (default)
# limit_by = "header"        - Rate limit by custom header value (requires limit_by_header)
# limit_by = "route"         - Rate limit by route path (all clients share the same limit)
# limit_by = "combined"      - Rate limit by IP + route combination

# Example with header-based rate limiting:
# limit_by = "header"
# limit_by_header = "X-API-Key"  # Header name to use for rate limiting

# Routes with per-route rate limiting overrides
[[routes]]
prefix = "/api"
backend = "backend-a:9000"
fingerprinting = true

# Override global rate limit for this route
[routes.rate_limit]
enabled = true
requests_per_second = 50   # Lower limit for API endpoints
burst = 100
limit_by = "combined"       # Per-IP, per-route limiting

[[routes]]
prefix = "/public"
backend = "backend-b:9000"
fingerprinting = false

# Disable rate limiting for this specific route
[routes.rate_limit]
enabled = false

[[routes]]
prefix = "/api/premium"
backend = "backend-a:9000"

# Higher limits for premium endpoints
[routes.rate_limit]
enabled = true
requests_per_second = 200
burst = 400
limit_by = "header"
limit_by_header = "X-API-Key"  # Rate limit by API key

[[routes]]
prefix = "/health"
backend = "backend-a:9000"

# No rate limiting for health check endpoint
[routes.rate_limit]
enabled = false

# TLS configuration (optional)
[tls]
cert_path = "/config/certs/server.crt"
key_path = "/config/certs/server.key"
alpn = ["h2", "http/1.1"]

# Logging configuration
[logging]
level = "info"
show_target = false

# Timeout configuration
[timeout]
connect_ms = 5000
idle_ms = 60000
shutdown_secs = 30
tls_handshake_secs = 15
connection_handling_secs = 300

[timeout.keep_alive]
enabled = true
timeout_secs = 60

# Telemetry configuration (optional)
[telemetry]
metrics_port = 9090  # Prometheus metrics endpoint

# Rate Limiting Behavior:
# 
# When a client exceeds the rate limit, they will receive:
# - HTTP Status: 429 Too Many Requests
# - Headers:
#   * X-Rate-Limit-Limit: Maximum requests allowed
#   * X-Rate-Limit-Remaining: 0
#   * X-RateLimit-Reset: Seconds until rate limit resets
#
# Example response:
# HTTP/1.1 429 Too Many Requests
# X-Rate-Limit-Limit: 50
# X-Rate-Limit-Remaining: 0
# X-RateLimit-Reset: 1
# Content-Length: 17
#
# Too Many Requests

# Rate Limiting Strategies:
#
# 1. By IP (limit_by = "ip"):
#    - Tracks requests per client IP address
#    - Uses X-Forwarded-For header if present (for proxied requests)
#    - Falls back to connection IP address
#    - Best for: Public APIs, preventing DDoS attacks
#
# 2. By Header (limit_by = "header"):
#    - Tracks requests by custom header value (e.g., API key, user ID)
#    - Requires limit_by_header to specify header name
#    - Falls back to IP if header not present
#    - Best for: Authenticated APIs, per-user/per-key rate limiting
#
# 3. By Route (limit_by = "route"):
#    - All clients share the same rate limit for a route
#    - Useful for protecting specific endpoints
#    - Best for: Resource-intensive endpoints, backend protection
#
# 4. Combined (limit_by = "combined"):
#    - Tracks requests by IP + route combination
#    - Provides per-IP limits that are also route-specific
#    - Best for: Fine-grained control, multi-tenant applications

# Performance Notes:
#
# - Rate limiting uses in-memory data structures (no external dependencies)
# - Lock-free algorithms for high-performance operation
# - Minimal overhead (~1-2% latency increase)
# - Memory usage: ~100 bytes per unique key (IP/header/route)
# - Sliding window algorithm for accurate rate tracking
# - Automatic cleanup of expired entries

# Best Practices:
#
# 1. Set burst > requests_per_second to handle traffic spikes
#    - Typical ratio: burst = 2 * requests_per_second
#
# 2. Choose appropriate limit_by strategy:
#    - Public APIs: "ip" or "combined"
#    - Authenticated APIs: "header" with API key
#    - Shared resources: "route"
#
# 3. Monitor rate limit metrics:
#    - Use telemetry.metrics_port to expose Prometheus metrics
#    - Track rate_limited errors in logs
#
# 4. Test your limits:
#    - Use load testing tools (e.g., wrk, hey, ab)
#    - Verify 429 responses are returned correctly
#    - Check that legitimate traffic is not affected
#
# 5. Adjust limits based on backend capacity:
#    - Set limits below backend saturation point
#    - Consider connection pooling and backend concurrency
#    - Monitor backend response times and error rates
